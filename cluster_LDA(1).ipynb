{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.models.doc2vec import Doc2Vec, LabeledSentence\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from pprint import pprint\n",
    "import time\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stopword():\n",
    "    #stopword_path = 'F:\\\\编程练习\\\\Jupyter notebook\\\\文本文件夹\\\\博客爬虫分析文章\\\\停用词表.txt'\n",
    "    #stopword_path = 'F:\\\\学习用夹\\\\大三下学期\\\\综合课程设计\\\\实验数据\\\\停用词库.txt'\n",
    "    stopword_path = 'F:\\\\学习用夹\\\\大三下学期\\\\综合课程设计\\\\实验数据\\\\scu_stopwords.txt'\n",
    "    f_stop = open(stopword_path,'r',encoding='UTF-8')\n",
    "    sw = [line.strip() for line in f_stop]\n",
    "    f_stop.close()\n",
    "    stopword_extend = ['\\n','湖北','武汉','病毒','中国',\n",
    "                       '兰州','甘肃','我国','北京','微博','正文',\n",
    "                       '收起','肖战','消费','复工','复产','全文','消费者','疫情','网页','链接','美国','浙江','杭州','新冠','市场']\n",
    "    sw.extend(stopword_extend)\n",
    "    return sw\n",
    "\n",
    "\n",
    "def jieba_text(data_path):\n",
    "    stopwords = load_stopword()\n",
    "    df = pd.read_excel(data_path)\n",
    "    #df = pd.read_csv(data_path)\n",
    "    text_seg_list = []\n",
    "    for index,row in df.iterrows():\n",
    "        #fileId = row['id']\n",
    "        fileContent = row['内容']\n",
    "        segs = jieba.analyse.textrank(fileContent,topK=20,withWeight=False,allowPOS=('ns','n','nr','nt','vn')) \n",
    "        segments_list = []\n",
    "        for seg in segs:\n",
    "            if seg not in stopwords and len(seg) > 1:\n",
    "                segments_list.append(seg)\n",
    "        text_seg_list.append(segments_list)\n",
    "    return text_seg_list\n",
    "\n",
    "    \n",
    "#导出csv文件\n",
    "def dataToCsv(file,df):\n",
    "    file_data = df\n",
    "    file_data.to_csv(file,index=False)\n",
    "    print('csv文件已生成在：{}'.format(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA(text_seg_list,topic_num):\n",
    "    # 建立字典\n",
    "    dictionary = corpora.Dictionary(text_seg_list)\n",
    "    V = len(dictionary)\n",
    "\n",
    "    # 转换文本数据为索引，并计数\n",
    "    corpus = [dictionary.doc2bow(text) for text in text_seg_list]\n",
    "\n",
    "    # 计算tf-idf值\n",
    "    corpus_tfidf = models.TfidfModel(corpus)[corpus]\n",
    "\n",
    "    # 训练模型\n",
    "    lda = models.LdaModel(corpus_tfidf, num_topics=topic_num, id2word=dictionary)\n",
    "    #alpha=0.01, eta=0.01, minimum_probability=0.001,update_every=1, chunksize=100, passes=1\n",
    "    Perplexity = lda.log_perplexity(corpus_tfidf)\n",
    "\n",
    "    num_show_term = 10  # 每个主题显示几个词\n",
    "    #print('结果：每个主题的词分布：')\n",
    "    lda_topic = {'1':[],'2':[],'3':[],'4':[],'5':[],'6':[],'7':[],'8':[],'9':[],'10':[]}\n",
    "    lda_topic_prob = []\n",
    "    for topic_id in range(topic_num):\n",
    "        #print('主题#%d：\\t' % topic_id)\n",
    "        term_distribute_all = lda.get_topic_terms(topicid=topic_id)\n",
    "        term_distribute = term_distribute_all[:num_show_term]\n",
    "        term_distribute = np.array(term_distribute)\n",
    "        term_id = term_distribute[:, 0].astype(np.int)\n",
    "        #print('词：\\t', )\n",
    "        i = 0\n",
    "        for t in term_id:\n",
    "            i += 1\n",
    "            #print(dictionary.id2token[t], )\n",
    "            lda_topic[str(i)].append(dictionary.id2token[t])\n",
    "        #print('\\n概率：\\t', term_distribute[:, 1])\n",
    "        lda_topic_prob.append(term_distribute[:, 1])\n",
    "    #print(lda.print_topics(5))\n",
    "    \n",
    "    #lda可视化\n",
    "    vis_data = pyLDAvis.gensim.prepare(lda,corpus_tfidf,dictionary)\n",
    "    pyLDAvis.show(vis_data,open_browser = False)\n",
    "    \n",
    "    #输出每个list所属的类别\n",
    "    #topic_list=[]\n",
    "    #for topics in lda.get_document_topics(corpus)[:]:\n",
    "        #for topic in topics:\n",
    "            #topic_list.append(topic[0])\n",
    "   # corpus_topic_df = pd.DataFrame({'content':text_seg_list,'topic':topic_list})\n",
    "    #dataToCsv('F:\\\\学习用夹\\\\大三下学期\\\\综合课程设计\\\\实验数据\\\\LDA结果\\\\LDA文本分类',corpus_topic_df)\n",
    "    \n",
    "    return lda_topic,lda_topic_prob,Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc2vec与K-means聚类\n",
    "TaggededDocument = gensim.models.doc2vec.TaggedDocument\n",
    " \n",
    "    \n",
    "def get_train(text_seg_list):\n",
    "    x_train = []\n",
    "    for i, text in enumerate(text_seg_list):\n",
    "        document = TaggededDocument(text, tags=[i])\n",
    "        x_train.append(document)\n",
    "    return x_train\n",
    " \n",
    "    \n",
    "def train(x_train, size=200, epoch_num=1):\n",
    "    model_dm = Doc2Vec(x_train, min_count=1, window = 3, size = size, sample=1e-3, negative=5, workers=4)\n",
    "    model_dm.train(x_train, total_examples=model_dm.corpus_count, epochs=100)\n",
    "    #model_dm.save('model/model_dm')\n",
    "    return model_dm\n",
    " \n",
    "    \n",
    "def cluster(x_train,model_dm,n_clu):\n",
    "    infered_vectors_list = []\n",
    "    for text, label in x_train:\n",
    "        vector = model_dm.infer_vector(text)\n",
    "        infered_vectors_list.append(vector)\n",
    " \n",
    "    kmean_model = KMeans(n_clusters=n_clu)\n",
    "    kmean_model.fit(infered_vectors_list)\n",
    "    labels= kmean_model.predict(infered_vectors_list)\n",
    "    return labels,infered_vectors_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_1 = 'F:\\\\学习用夹\\\\大三下学期\\\\综合课程设计\\\\实验数据\\\\复产内容信息2020-05-10.xls'\n",
    "data_path_2 = 'F:\\\\学习用夹\\\\大三下学期\\\\综合课程设计\\\\实验数据\\\\复工内容信息2020-05-10.xlsx'\n",
    "data_path_3 = 'F:\\\\学习用夹\\\\大三下学期\\\\综合课程设计\\\\实验数据\\\\消费内容信息2020-05-10.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\MU_XIA~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.589 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "C:\\Users\\Mu_Xiaobai\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:574: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    }
   ],
   "source": [
    "#建立训练集\n",
    "text_seg_list = jieba_text(data_path_3)\n",
    "x_train = get_train(text_seg_list)\n",
    "model_dm = train(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#轮廓系数评估\n",
    "infered_vectors_list = []\n",
    "for text, label in x_train:\n",
    "    vector = model_dm.infer_vector(text)\n",
    "    infered_vectors_list.append(vector)\n",
    "\n",
    "def get_cluster_n(infered_vectors_list,model_dm,n_clu):\n",
    "    kmean_model = KMeans(n_clusters=n_clu)\n",
    "    kmean_model.fit(infered_vectors_list)\n",
    "    labels = kmean_model.predict(infered_vectors_list)\n",
    "    print(metrics.silhouette_score(infered_vectors_list, kmean_model.labels_, metric='cosine'))\n",
    "\n",
    "#寻找3到20里的轮廓系数，用系数较高的值做聚类中心\n",
    "for i in range(3,20):\n",
    "    get_cluster_n(infered_vectors_list,model_dm,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#根据聚类中心进行K-Means聚类\n",
    "labels,infered_vectors_list = cluster(x_train,model_dm,3)   #第二个系数选择聚类中心\n",
    "clu_df = pd.DataFrame({'内容':text_seg_list})\n",
    "clu_df['labels'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv文件已生成在：F:\\学习用夹\\大三下学期\\综合课程设计\\实验数据\\聚类结果\\0.csv\n",
      "csv文件已生成在：F:\\学习用夹\\大三下学期\\综合课程设计\\实验数据\\聚类结果\\1.csv\n",
      "csv文件已生成在：F:\\学习用夹\\大三下学期\\综合课程设计\\实验数据\\聚类结果\\2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mu_Xiaobai\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:35: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "#结果存入相应文件夹\n",
    "save_path = 'F:\\\\学习用夹\\\\大三下学期\\\\综合课程设计\\\\实验数据\\\\聚类结果\\\\'   #保存聚类结果的文件夹\n",
    "clu_gp = clu_df.groupby('labels')\n",
    "\n",
    "cluster_lists = []\n",
    "for clu_id in clu_gp.groups.keys():\n",
    "    dataToCsv(save_path + str(clu_id) + '.csv', clu_gp.get_group(clu_id)['内容'])  #在保存的csv文件里查看聚类结果好坏\n",
    "    cluster_lists.append(clu_gp.get_group(clu_id)['内容'].values.tolist())   #cluster_lists[n],n为聚类中心数，在lda中用此列表分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "产品          762\n",
      "行业          615\n",
      "公司          614\n",
      "科技          590\n",
      "汽车          583\n",
      "新闻          539\n",
      "投资          528\n",
      "服务          527\n",
      "生产          496\n",
      "社会          473\n"
     ]
    }
   ],
   "source": [
    "#聚类后的词频分析,根据高频词判断聚类质量\n",
    "def word_count(cluster_list):\n",
    "    counts = {}\n",
    "    words = []\n",
    "    for w_list in cluster_list:\n",
    "        for w in w_list:\n",
    "            words.append(w)\n",
    "    for word in words:\n",
    "        counts[word] = counts.get(word, 0) + 1\n",
    "    items = list(counts.items())\n",
    "    items.sort(key=lambda x:x[1], reverse=True)\n",
    "    for i in range(10):\n",
    "        word, count = items[i]\n",
    "        print(\"{0:<10}{1:>5}\".format(word, count))\n",
    "\n",
    "word_count(cluster_lists[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mu_Xiaobai\\Anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "arrays must all be same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-4d2ab5d58c41>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#对相应文件做lda分析\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlda_topic\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlda_topic_prob\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mPerplexity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLDA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcluster_lists\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;31m#第二个系数为lda主题个数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf_topic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda_topic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf_topic_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda_topic_prob\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'2'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'3'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'4'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'5'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'6'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'7'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'8'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'9'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'10'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_topic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-22bfb4a7db42>\u001b[0m in \u001b[0;36mLDA\u001b[1;34m(text_seg_list, topic_num)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtopic\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtopics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[0mtopic_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[0mcorpus_topic_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'content'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtext_seg_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'topic'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtopic_list\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m     \u001b[0mdataToCsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'F:\\\\学习用夹\\\\大三下学期\\\\综合课程设计\\\\实验数据\\\\LDA结果\\\\LDA文本分类'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcorpus_topic_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    409\u001b[0m             )\n\u001b[0;32m    410\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 411\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    412\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[1;34m(data, index, columns, dtype)\u001b[0m\n\u001b[0;32m    255\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m         ]\n\u001b[1;32m--> 257\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[1;31m# figure out the index, if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36mextract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    366\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 368\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"arrays must all be same length\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: arrays must all be same length"
     ]
    }
   ],
   "source": [
    "#对相应文件做lda分析\n",
    "lda_topic,lda_topic_prob,Perplexity = LDA(cluster_lists[2],3)    #第二个系数为lda主题个数\n",
    "df_topic = pd.DataFrame(lda_topic)\n",
    "df_topic_prob = pd.DataFrame(lda_topic_prob,columns=['1','2','3','4','5','6','7','8','9','10'])\n",
    "print(df_topic)\n",
    "print(df_topic_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
