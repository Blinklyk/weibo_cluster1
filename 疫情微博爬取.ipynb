{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "收集开始...\n",
      "当前关键词：消费\n",
      "正在收集第1天页码链接\n",
      "正在收集第2天页码链接\n",
      "正在收集第3天页码链接\n",
      "正在收集第4天页码链接\n",
      "正在收集第5天页码链接\n",
      "正在收集第6天页码链接\n",
      "正在收集第7天页码链接\n",
      "正在收集第8天页码链接\n",
      "正在收集第9天页码链接\n",
      "正在收集第10天页码链接\n",
      "正在收集第11天页码链接\n",
      "正在收集第12天页码链接\n",
      "正在收集第13天页码链接\n",
      "正在收集第14天页码链接\n",
      "正在收集第15天页码链接\n",
      "正在收集第16天页码链接\n",
      "正在收集第17天页码链接\n",
      "正在收集第18天页码链接\n",
      "正在收集第19天页码链接\n",
      "正在收集第20天页码链接\n",
      "已收集页数： 1\n",
      "已收集页数： 2\n",
      "已收集页数： 3\n",
      "已收集页数： 4\n",
      "已收集页数： 5\n",
      "已收集页数： 6\n",
      "已收集页数： 7\n",
      "已收集页数： 8\n",
      "已收集页数： 9\n",
      "已收集页数： 10\n",
      "已收集页数： 11\n",
      "已收集页数： 12\n",
      "已收集页数： 13\n",
      "已收集页数： 14\n",
      "已收集页数： 15\n",
      "已收集页数： 16\n",
      "已收集页数： 17\n",
      "已收集页数： 18\n",
      "已收集页数： 19\n",
      "已收集页数： 20\n",
      "已收集页数： 21\n",
      "已收集页数： 22\n",
      "已收集页数： 23\n",
      "已收集页数： 24\n",
      "已收集页数： 25\n",
      "已收集页数： 26\n",
      "已收集页数： 27\n",
      "已收集页数： 28\n",
      "已收集页数： 29\n",
      "已收集页数： 30\n",
      "已收集页数： 31\n",
      "已收集页数： 32\n",
      "已收集页数： 33\n",
      "已收集页数： 34\n",
      "已收集页数： 35\n",
      "已收集页数： 36\n",
      "已收集页数： 37\n",
      "已收集页数： 38\n",
      "已收集页数： 39\n",
      "已收集页数： 40\n",
      "已收集页数： 41\n",
      "已收集页数： 42\n",
      "已收集页数： 43\n",
      "已收集页数： 44\n",
      "已收集页数： 45\n",
      "已收集页数： 46\n",
      "已收集页数： 47\n",
      "已收集页数： 48\n",
      "已收集页数： 49\n",
      "已收集页数： 50\n",
      "已收集页数： 51\n",
      "已收集页数： 52\n",
      "已收集页数： 53\n",
      "已收集页数： 54\n",
      "已收集页数： 55\n",
      "已收集页数： 56\n",
      "已收集页数： 57\n",
      "已收集页数： 58\n",
      "已收集页数： 59\n",
      "已收集页数： 60\n",
      "已收集页数： 61\n",
      "已收集页数： 62\n",
      "已收集页数： 63\n",
      "已收集页数： 64\n",
      "已收集页数： 65\n",
      "已收集页数： 66\n",
      "已收集页数： 67\n",
      "已收集页数： 68\n",
      "已收集页数： 69\n",
      "已收集页数： 70\n",
      "已收集页数： 71\n",
      "已收集页数： 72\n",
      "已收集页数： 73\n",
      "已收集页数： 74\n",
      "已收集页数： 75\n",
      "已收集页数： 76\n",
      "已收集页数： 77\n",
      "已收集页数： 78\n",
      "已收集页数： 79\n",
      "已收集页数： 80\n",
      "已收集页数： 81\n",
      "已收集页数： 82\n",
      "已收集页数： 83\n",
      "已收集页数： 84\n",
      "已收集页数： 85\n",
      "已收集页数： 86\n",
      "已收集页数： 87\n",
      "已收集页数： 88\n",
      "已收集页数： 89\n",
      "已收集页数： 90\n",
      "已收集页数： 91\n",
      "已收集页数： 92\n",
      "已收集页数： 93\n",
      "已收集页数： 94\n",
      "已收集页数： 95\n",
      "已收集页数： 96\n",
      "已收集页数： 97\n",
      "已收集页数： 98\n",
      "已收集页数： 99\n",
      "已收集页数： 100\n",
      "已收集页数： 101\n",
      "已收集页数： 102\n",
      "已收集页数： 103\n",
      "已收集页数： 104\n",
      "已收集页数： 105\n",
      "已收集页数： 106\n",
      "已收集页数： 107\n",
      "已收集页数： 108\n",
      "已收集页数： 109\n",
      "已收集页数： 110\n",
      "已收集页数： 111\n",
      "已收集页数： 112\n",
      "已收集页数： 113\n",
      "已收集页数： 114\n",
      "已收集页数： 115\n",
      "已收集页数： 116\n",
      "已收集页数： 117\n",
      "已收集页数： 118\n",
      "已收集页数： 119\n",
      "已收集页数： 120\n",
      "已收集页数： 121\n",
      "已收集页数： 122\n",
      "已收集页数： 123\n",
      "已收集页数： 124\n",
      "已收集页数： 125\n",
      "已收集页数： 126\n",
      "已收集页数： 127\n",
      "已收集页数： 128\n",
      "已收集页数： 129\n",
      "已收集页数： 130\n",
      "已收集页数： 131\n",
      "已收集页数： 132\n",
      "已收集页数： 133\n",
      "已收集页数： 134\n",
      "已收集页数： 135\n",
      "已收集页数： 136\n",
      "已收集页数： 137\n",
      "已收集页数： 138\n",
      "已收集页数： 139\n",
      "已收集页数： 140\n",
      "已收集页数： 141\n",
      "已收集页数： 142\n",
      "已收集页数： 143\n",
      "已收集页数： 144\n",
      "已收集页数： 145\n",
      "已收集页数： 146\n",
      "已收集页数： 147\n",
      "已收集页数： 148\n",
      "已收集页数： 149\n",
      "已收集页数： 150\n",
      "已收集页数： 151\n",
      "已收集页数： 152\n",
      "已收集页数： 153\n",
      "已收集页数： 154\n",
      "已收集页数： 155\n",
      "已收集页数： 156\n",
      "已收集页数： 157\n",
      "已收集页数： 158\n",
      "已收集页数： 159\n",
      "已收集页数： 160\n",
      "已收集页数： 161\n",
      "已收集页数： 162\n",
      "已收集页数： 163\n",
      "已收集页数： 164\n",
      "已收集页数： 165\n",
      "已收集页数： 166\n",
      "已收集页数： 167\n",
      "已收集页数： 168\n",
      "已收集页数： 169\n",
      "已收集页数： 170\n",
      "已收集页数： 171\n",
      "已收集页数： 172\n",
      "已收集页数： 173\n",
      "已收集页数： 174\n",
      "已收集页数： 175\n",
      "已收集页数： 176\n",
      "已收集页数： 177\n",
      "已收集页数： 178\n",
      "已收集页数： 179\n",
      "已收集页数： 180\n",
      "已收集页数： 181\n",
      "已收集页数： 182\n",
      "已收集页数： 183\n",
      "已收集页数： 184\n",
      "已收集页数： 185\n",
      "已收集页数： 186\n",
      "已收集页数： 187\n",
      "已收集页数： 188\n",
      "已收集页数： 189\n",
      "已收集页数： 190\n",
      "已收集页数： 191\n",
      "已收集页数： 192\n",
      "已收集页数： 193\n",
      "已收集页数： 194\n",
      "已收集页数： 195\n",
      "已收集页数： 196\n",
      "已收集页数： 197\n",
      "已收集页数： 198\n",
      "已收集页数： 199\n",
      "已收集页数： 200\n",
      "已收集页数： 201\n",
      "已收集页数： 202\n",
      "已收集页数： 203\n",
      "已收集页数： 204\n",
      "已收集页数： 205\n",
      "已收集页数： 206\n",
      "已收集页数： 207\n",
      "已收集页数： 208\n",
      "已收集页数： 209\n",
      "已收集页数： 210\n",
      "已收集页数： 211\n",
      "已收集页数： 212\n",
      "已收集页数： 213\n",
      "已收集页数： 214\n",
      "已收集页数： 215\n",
      "已收集页数： 216\n",
      "已收集页数： 217\n",
      "已收集页数： 218\n",
      "已收集页数： 219\n",
      "已收集页数： 220\n",
      "已收集页数： 221\n",
      "已收集页数： 222\n",
      "已收集页数： 223\n",
      "已收集页数： 224\n",
      "已收集页数： 225\n",
      "已收集页数： 226\n",
      "已收集页数： 227\n",
      "已收集页数： 228\n",
      "已收集页数： 229\n",
      "已收集页数： 230\n",
      "已收集页数： 231\n",
      "已收集页数： 232\n",
      "已收集页数： 233\n",
      "已收集页数： 234\n",
      "已收集页数： 235\n",
      "已收集页数： 236\n",
      "已收集页数： 237\n",
      "已收集页数： 238\n",
      "已收集页数： 239\n",
      "已收集页数： 240\n",
      "已收集页数： 241\n",
      "已收集页数： 242\n",
      "已收集页数： 243\n",
      "已收集页数： 244\n",
      "已收集页数： 245\n",
      "已收集页数： 246\n",
      "已收集页数： 247\n",
      "已收集页数： 248\n",
      "已收集页数： 249\n",
      "已收集页数： 250\n",
      "已收集页数： 251\n",
      "已收集页数： 252\n",
      "已收集页数： 253\n",
      "已收集页数： 254\n",
      "已收集页数： 255\n",
      "已收集页数： 256\n",
      "已收集页数： 257\n",
      "已收集页数： 258\n",
      "已收集页数： 259\n",
      "已收集页数： 260\n",
      "已收集页数： 261\n",
      "已收集页数： 262\n",
      "已收集页数： 263\n",
      "已收集页数： 264\n",
      "已收集页数： 265\n",
      "已收集页数： 266\n",
      "已收集页数： 267\n",
      "已收集页数： 268\n",
      "已收集页数： 269\n",
      "已收集页数： 270\n",
      "已收集页数： 271\n",
      "已收集页数： 272\n",
      "已收集页数： 273\n",
      "已收集页数： 274\n",
      "已收集页数： 275\n",
      "已收集页数： 276\n",
      "已收集页数： 277\n",
      "已收集页数： 278\n",
      "已收集页数： 279\n",
      "已收集页数： 280\n",
      "已收集页数： 281\n",
      "已收集页数： 282\n",
      "已收集页数： 283\n",
      "已收集页数： 284\n",
      "已收集页数： 285\n",
      "已收集页数： 286\n",
      "已收集页数： 287\n",
      "已收集页数： 288\n",
      "已收集页数： 289\n",
      "已收集页数： 290\n",
      "已收集页数： 291\n",
      "已收集页数： 292\n",
      "已收集页数： 293\n",
      "已收集页数： 294\n",
      "已收集页数： 295\n",
      "已收集页数： 296\n",
      "已收集页数： 297\n",
      "已收集页数： 298\n",
      "已收集页数： 299\n",
      "已收集页数： 300\n",
      "已收集页数： 301\n",
      "已收集页数： 302\n",
      "已收集页数： 303\n",
      "已收集页数： 304\n",
      "已收集页数： 305\n",
      "已收集页数： 306\n",
      "已收集页数： 307\n",
      "已收集页数： 308\n",
      "已收集页数： 309\n",
      "已收集页数： 310\n",
      "已收集页数： 311\n",
      "已收集页数： 312\n",
      "已收集页数： 313\n",
      "已收集页数： 314\n",
      "已收集页数： 315\n",
      "已收集页数： 316\n",
      "已收集页数： 317\n",
      "已收集页数： 318\n",
      "已收集页数： 319\n",
      "已收集页数： 320\n",
      "已收集页数： 321\n",
      "已收集页数： 322\n",
      "已收集页数： 323\n",
      "已收集页数： 324\n",
      "已收集页数： 325\n",
      "已收集页数： 326\n",
      "已收集页数： 327\n",
      "已收集页数： 328\n",
      "已收集页数： 329\n",
      "已收集页数： 330\n",
      "已收集页数： 331\n",
      "已收集页数： 332\n",
      "已收集页数： 333\n",
      "微博内容录入成功\n",
      "已完成全部信息收集\n"
     ]
    }
   ],
   "source": [
    "import requests #一些关键包\n",
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import xlwt\n",
    "import xlrd\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "def get_headers(): #得到伪造的文件头（考虑到验证问题，请自行采集cookie）\n",
    "    headers = {\n",
    "                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "                'Accept-Language': 'zh-CN,zh;q=0.8',\n",
    "                'Connection': 'keep-alive',\n",
    "                'Accept-Encoding': 'gzip, deflate, br',\n",
    "                'Cache-Control': 'max-age=0',\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36'\n",
    "               }\n",
    "    headers['Cookie']=''\n",
    "    return headers\n",
    "def GetMiddleStr(content,startStr,endStr): #对用户页面的HTML进行过滤\n",
    "    startIndex = content.find(startStr)\n",
    "    if startIndex>=0:\n",
    "        startIndex += len(startStr)\n",
    "    endIndex = content.find(endStr,startIndex)\n",
    "    return content[startIndex:endIndex].replace(\"\\\\\",\"\")\n",
    "def GetMiddleStr_(content,index): #对用户页面的HTML进行过滤\n",
    "    find_str='<script>FM.view({\"ns\":\"'\n",
    "    find_str_='\"html\":\"'\n",
    "    end_str='})</script>'\n",
    "    startIndex=0\n",
    "    for i in range(index):\n",
    "        startIndex = content.find(find_str,startIndex+1)\n",
    "    startIndex=content.find(find_str_,startIndex)+len(find_str_)\n",
    "    endIndex = content.find(end_str,startIndex)\n",
    "    return content[startIndex:endIndex].replace(\"\\\\\",\"\")\n",
    "def get_all_pages_link(keyword):  #得到当天一个关键词对应的全部热点页面链接\n",
    "    links=[]\n",
    "    url='https://s.weibo.com/weibo?q='+keyword+'&xsort=hot&suball=1&timescope=custom:2020-'+month+'-'+day+'-0:2020-'+month+'-'+day+'-23&Refer=g'\n",
    "    first_page=requests.get(url,headers=get_headers()).text\n",
    "    first_page_soup = BeautifulSoup(first_page, 'lxml')\n",
    "    page_inf=first_page_soup.find_all('ul',class_=\"s-scroll\")\n",
    "    for each in page_inf[0].find_all('a'):\n",
    "        links.append(each.get('href'))\n",
    "    return links\n",
    "def get_all_pages_link_by_date(keyword):  #按日期得到一个关键词对应的全部热点页面链接（临时编，推荐调用datetime模块）\n",
    "    links=[]\n",
    "    for i in range(60):\n",
    "        time.sleep(2)\n",
    "        print('正在收集第'+str(i+1)+'天页码链接')\n",
    "        month='02'\n",
    "        day=i+1\n",
    "        if i+1>31and i+1<61:\n",
    "            month='03'\n",
    "            day=i+1-29\n",
    "        elif i+1>=61:\n",
    "            month='04'\n",
    "            day=i+1-60\n",
    "        else:\n",
    "            pass\n",
    "        if day<10:\n",
    "            day='0'+str(day)\n",
    "        else:\n",
    "            day=str(day)\n",
    "        url='https://s.weibo.com/weibo?q='+keyword+'&xsort=hot&suball=1&timescope=custom:2020-'+month+'-'+day+'-0:2020-'+month+'-'+day+'-23&Refer=g'\n",
    "        first_page=requests.get(url,headers=get_headers()).text\n",
    "        first_page_soup = BeautifulSoup(first_page, 'lxml')\n",
    "        page_inf=first_page_soup.find_all('ul',class_=\"s-scroll\")\n",
    "        if len(page_inf)==0:\n",
    "            links.append('weibo?q='+keyword+'&xsort=hot&suball=1&timescope=custom:2020-'+month+'-'+day+'-0:2020-'+month+'-'+day+'-23&Refer=g')\n",
    "        else:\n",
    "            for each in page_inf[0].find_all('a'):\n",
    "                links.append(each.get('href'))\n",
    "    return links\n",
    "def get_user_info(link_inf):  #得到一条微博的用户信息\n",
    "    user_inf_=''\n",
    "    user_type=''\n",
    "    user_types=['官方认证用户','普通用户','个人认证用户','企业认证用户','政府认证用户','明星、自媒体、博主等大V用户','明星、自媒体、博主等大V用户','明星、自媒体、博主等大V用户','明星、自媒体、博主等大V用户']\n",
    "    type_num=['100206','other','100505','100606','100106','100406','100605','100306','103505']\n",
    "    try_times=3\n",
    "    if link_inf[0]=='u':\n",
    "        link_inf=link_inf[2:]\n",
    "    for n in range(0,len(user_types)):\n",
    "        for i in range(try_times):\n",
    "            try:\n",
    "                if n==1:\n",
    "                    user_inf=requests.get('https://weibo.com/u/'+link_inf+'?refer_flag=1001030103_&is_hot=1',headers=get_headers(),timeout=15)\n",
    "                else:\n",
    "                    user_inf=requests.get('https://weibo.com/p/'+type_num[n]+link_inf+'/home?from=page_'+type_num[n]+'&mod=TAB&is_hot=1',headers=get_headers(),timeout=15)\n",
    "                if user_inf.status_code == 200:\n",
    "                    break\n",
    "            except:\n",
    "                print('第'+str(i+1)+'次获取信息失败')\n",
    "                if i==2:\n",
    "                    print('已跳过，继续执行')\n",
    "                    return 'null','null','null','null'\n",
    "        user_inf=user_inf.text\n",
    "        #print(user_inf)\n",
    "        user_inf_=GetMiddleStr(str(user_inf),r'<script>FM.view({\"ns\":\"\",\"domid\":\"Pl_Core_T8CustomTriColumn__3\",\"css\":[\"style/css/module/pagecard/PCD_counter.css?version=bd9471367eec98f8\"],\"html\":\"',r'})</script>')\n",
    "        if user_inf_=='':\n",
    "            continue\n",
    "        else:\n",
    "            user_type=user_types[n]\n",
    "            break\n",
    "    if user_inf_=='':\n",
    "        user_type='其他'\n",
    "        return 'null','null','null',user_type\n",
    "    user_inf_soup=BeautifulSoup(user_inf_,'lxml')\n",
    "    forward=user_inf_soup.find_all('strong')[1].string\n",
    "    weibo_num=user_inf_soup.find_all('strong')[2].string\n",
    "    user_inf_=GetMiddleStr_(str(user_inf),5)\n",
    "    user_inf_soup=BeautifulSoup(user_inf_,'lxml')\n",
    "    if user_inf_=='':\n",
    "        print('tag标签缺失')\n",
    "        return forward,weibo_num,'null',user_type\n",
    "    tag=user_inf_soup.find('div',class_='pf_intro').get('title')\n",
    "    return forward,weibo_num,tag,user_type\n",
    "def get_all_user_inf(link_inf,keyword):#得到全部的用户信息\n",
    "    index=0\n",
    "    fans=[]\n",
    "    weibo_num=[]\n",
    "    tags=[]\n",
    "    users_type=[]\n",
    "    link_index=link_inf\n",
    "    norepeat=[]#去重\n",
    "    np.array(range(0,len(link_inf)))\n",
    "    norepeat.append(link_inf[0])\n",
    "    for i in range(0,len(link_inf)):\n",
    "        repeat_num=-1\n",
    "        repeat=0\n",
    "        each_link=link_inf[i]\n",
    "        for j in range(0,len(norepeat)):\n",
    "            if each_link==norepeat[j]:\n",
    "                repeat_num=j\n",
    "                repeat=1\n",
    "        if repeat==1:\n",
    "            link_index[i]=repeat_num\n",
    "        else:\n",
    "            norepeat.append(each_link)\n",
    "            link_index[i]=len(norepeat)-1 \n",
    "    #print(len(norepeat))\n",
    "    print('开始逐条获取用户信息('+keyword+')...共',len(norepeat),'条')\n",
    "    for each in norepeat:\n",
    "        time.sleep(1)\n",
    "        index=index+1\n",
    "        print('正在采集第',index,'个用户信息')\n",
    "        if index%100==0:\n",
    "            print(fans,weibo_num,tags,users_type)\n",
    "        try:\n",
    "            fan,weibo_num_,tag,user_type=get_user_info(str(each))\n",
    "        except BaseException:\n",
    "            print('出现错误，共收集',index,'条,如下所示：')\n",
    "            print(fans,weibo_num,tags,users_type)\n",
    "            fan,weibo_num_,tag,user_type='null','null','null','null'\n",
    "            time.sleep(20)\n",
    "            print('已跳过，继续执行')\n",
    "        fans.append(fan)\n",
    "        weibo_num.append(weibo_num_)\n",
    "        tags.append(tag)\n",
    "        users_type.append(user_type)\n",
    "    print('用户信息采集完成')\n",
    "    workbook=xlwt.Workbook(encoding='utf-8')#把原始信息输出到excel\n",
    "    booksheet=workbook.add_sheet('Sheet 1', cell_overwrite_ok=True)\n",
    "    for i in range(0,len(link_inf)):\n",
    "        booksheet.write(i, 0, fans[link_index[i]])\n",
    "        booksheet.write(i, 1, weibo_num[link_index[i]])\n",
    "        booksheet.write(i, 2, tags[link_index[i]])\n",
    "        booksheet.write(i, 3, users_type[link_index[i]])\n",
    "    workbook.save('用户信息'+keyword+str(time.strftime('%Y-%m-%d',time.localtime(time.time())))+'.xls')\n",
    "    print('用户信息录入成功')\n",
    "def get_keyword_all_inf(page_links,keyword): #得到当天一个关键词下的全部信息并输出到excel\n",
    "    index=0\n",
    "    index_=0\n",
    "    start_time=time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time()))\n",
    "    inf=[]\n",
    "    names=[]\n",
    "    times=[]\n",
    "    contents=[]\n",
    "    forwards=[]\n",
    "    comments=[]\n",
    "    likes=[]\n",
    "    link_inf=[]\n",
    "    page=[]\n",
    "    for each in page_links:\n",
    "        time.sleep(1)\n",
    "        weibo_inf=requests.get('https://s.weibo.com/'+each,headers=get_headers()).text\n",
    "        index_=index_+1\n",
    "        print('已收集页数：',index_)\n",
    "        inf.append(weibo_inf)\n",
    "    print('已获取所有微博内容')\n",
    "    print('开始逐条处理信息...')\n",
    "    for n in range(0,len(inf)):\n",
    "        full_num=0\n",
    "        str_=[]\n",
    "        full_str=[]\n",
    "        contents_list=[]\n",
    "        full=[]\n",
    "        weibo_inf_soup=BeautifulSoup(inf[n], 'lxml')\n",
    "        num=len(weibo_inf_soup.find_all('div',class_='card-feed'))\n",
    "        names_list=weibo_inf_soup.find_all('a',class_='name')\n",
    "        times_list=weibo_inf_soup.find_all('p',class_='from')\n",
    "        forwards_list=weibo_inf_soup.find_all('a',attrs={\"action-type\": \"feed_list_forward\"})\n",
    "        comments_list=weibo_inf_soup.find_all('a',attrs={\"action-type\": \"feed_list_comment\"})\n",
    "        likes_list=weibo_inf_soup.find_all('a',attrs={\"action-type\": \"feed_list_like\"})\n",
    "        contents_list_=weibo_inf_soup.find_all('p',attrs={\"class\":\"txt\",\"node-type\": \"feed_list_content\"})\n",
    "        contents_list_full=weibo_inf_soup.find_all('p',attrs={\"class\":\"txt\",\"node-type\": \"feed_list_content_full\"})\n",
    "        for each in contents_list_:\n",
    "            str_.append(each.get_text())\n",
    "        for each in contents_list_full:\n",
    "            full_str.append(each.get_text())\n",
    "        for e in range(len(str_)):\n",
    "            f=0\n",
    "            cont=str_[e]\n",
    "            cont=cont[:-10]\n",
    "            for r in range(full_num,len(full_str)):\n",
    "                compared=full_str[r]\n",
    "                if cont in compared:\n",
    "                    f=1\n",
    "                    full_num=full_num+1\n",
    "                    full.append(r)\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "            if f==0:\n",
    "                full.append(-1)\n",
    "        for o in range(len(full)):\n",
    "            if full[o]==-1:\n",
    "                contents_list.append(str_[o])\n",
    "            else:\n",
    "                contents_list.append(full_str[full[o]])\n",
    "        for i in range(0,num):\n",
    "            index=index+1\n",
    "            print(index,names_list[i].string)\n",
    "            names.append(names_list[i].string)\n",
    "            times.append(times_list[i].a.string)\n",
    "            contents.append(contents_list[i])\n",
    "            forwards.append(forwards_list[i].string)\n",
    "            comments.append(comments_list[i].string)\n",
    "            likes.append(likes_list[i].em.string)\n",
    "            link_inf.append(GetMiddleStr(names_list[i].get('href'),'com/','?'))\n",
    "            page.append(int(n+1))\n",
    "    workbook=xlwt.Workbook(encoding='utf-8')#把原始信息输出到excel\n",
    "    booksheet=workbook.add_sheet('Sheet 1', cell_overwrite_ok=True)\n",
    "    for i in range(0,len(names)):\n",
    "        booksheet.write(i, 0, names[i])\n",
    "        booksheet.write(i, 1, contents[i])\n",
    "        booksheet.write(i, 2, times[i])\n",
    "        booksheet.write(i, 3, forwards[i])\n",
    "        booksheet.write(i, 4, str(comments[i]))\n",
    "        booksheet.write(i, 5, likes[i])\n",
    "        booksheet.write(i, 6, str(start_time))\n",
    "        booksheet.write(i, 7, link_inf[i])\n",
    "        booksheet.write(i, 8, page[i])\n",
    "    workbook.save(keyword+'内容信息'+str(time.strftime('%Y-%m-%d',time.localtime(time.time())))+'.xls')\n",
    "    print('微博内容录入成功')\n",
    "    return link_inf\n",
    "def get_keyword_all_inf_by_date(page_links,keyword): #按日期得到一个关键词下的全部信息并输出到excel\n",
    "    index=0\n",
    "    index_=0\n",
    "    start_time=time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time()))\n",
    "    names=[]\n",
    "    times=[]\n",
    "    contents=[]\n",
    "    forwards=[]\n",
    "    comments=[]\n",
    "    likes=[]\n",
    "    link_inf=[]\n",
    "    page=[]\n",
    "    symbols=[]\n",
    "    for each in page_links:\n",
    "        time.sleep(1)\n",
    "        weibo_inf=requests.get('https://s.weibo.com/'+each,headers=get_headers()).text\n",
    "        index=index+1\n",
    "        print('已收集页数：',index)\n",
    "        full_num=0\n",
    "        str_=[]\n",
    "        full_str=[]\n",
    "        contents_list=[]\n",
    "        full=[]\n",
    "        weibo_inf_soup=BeautifulSoup(weibo_inf, 'lxml')\n",
    "        num=len(weibo_inf_soup.find_all('div',class_='card-feed'))\n",
    "        names_list=weibo_inf_soup.find_all('a',class_='name')\n",
    "        times_list=weibo_inf_soup.find_all('p',class_='from')\n",
    "        forwards_list=weibo_inf_soup.find_all('a',attrs={\"action-type\": \"feed_list_forward\"})\n",
    "        comments_list=weibo_inf_soup.find_all('a',attrs={\"action-type\": \"feed_list_comment\"})\n",
    "        likes_list=weibo_inf_soup.find_all('a',attrs={\"action-type\": \"feed_list_like\"})\n",
    "        contents_list_=weibo_inf_soup.find_all('p',attrs={\"class\":\"txt\",\"node-type\": \"feed_list_content\"})\n",
    "        contents_list_full=weibo_inf_soup.find_all('p',attrs={\"class\":\"txt\",\"node-type\": \"feed_list_content_full\"})\n",
    "        for each in contents_list_:\n",
    "            str_.append(each.get_text())\n",
    "        for each in contents_list_full:\n",
    "            full_str.append(each.get_text())\n",
    "        for e in range(len(str_)):\n",
    "            f=0\n",
    "            cont=str_[e]\n",
    "            cont=cont[:-10]\n",
    "            for r in range(full_num,len(full_str)):\n",
    "                compared=full_str[r]\n",
    "                if cont in compared:\n",
    "                    f=1\n",
    "                    full_num=full_num+1\n",
    "                    full.append(r)\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "            if f==0:\n",
    "                full.append(-1)\n",
    "        for o in range(len(full)):\n",
    "            if full[o]==-1:\n",
    "                contents_list.append(str_[o])\n",
    "            else:\n",
    "                contents_list.append(full_str[full[o]])\n",
    "        for i in range(0,num):\n",
    "            index_=index_+1\n",
    "            names.append(names_list[i].string)\n",
    "            times.append(times_list[i].a.string)\n",
    "            contents.append(contents_list[i])\n",
    "            forwards.append(forwards_list[i].string)\n",
    "            comments.append(comments_list[i].string)\n",
    "            likes.append(likes_list[i].em.string)\n",
    "            link_inf.append(GetMiddleStr(names_list[i].get('href'),'com/','?'))\n",
    "            symbols.append(times_list[i].a.get('href'))\n",
    "            page.append(index_)\n",
    "    workbook=xlwt.Workbook(encoding='utf-8')#把原始信息输出到excel\n",
    "    booksheet=workbook.add_sheet('Sheet 1', cell_overwrite_ok=True)\n",
    "    for i in range(0,len(names)):\n",
    "        booksheet.write(i, 0, names[i])\n",
    "        booksheet.write(i, 1, contents[i])\n",
    "        booksheet.write(i, 2, times[i])\n",
    "        booksheet.write(i, 3, forwards[i])\n",
    "        booksheet.write(i, 4, str(comments[i]))\n",
    "        booksheet.write(i, 5, likes[i])\n",
    "        booksheet.write(i, 6, str(start_time))\n",
    "        booksheet.write(i, 7, link_inf[i])\n",
    "        booksheet.write(i, 8, page[i])\n",
    "        booksheet.write(i, 9, symbols[i])\n",
    "    workbook.save(keyword+'内容信息'+str(time.strftime('%Y-%m-%d',time.localtime(time.time())))+'.xls')\n",
    "    print('微博内容录入成功')\n",
    "#print(get_user_info('1887344341'))#调试用\n",
    "#print(get_keyword_all_inf(['/weibo?q=%E6%96%B0%E5%9E%8B%E5%86%A0%E7%8A%B6%E7%97%85%E6%AF%92&xsort=hot&Refer=hotmore&page=1'],'ss'))#调试用\n",
    "def main_func(): #主函数，控制要找的关键词（当天，收集用户信息）\n",
    "    keywords=['复产','复工','消费']\n",
    "    all_link=[]\n",
    "    print('收集开始...')\n",
    "    for each in keywords:\n",
    "        print('当前关键词：'+each)\n",
    "        all_link.append(get_keyword_all_inf(get_all_pages_link(each),each))\n",
    "    for i in range(0,len(keywords)):\n",
    "        get_all_user_inf(all_link[i],keywords[i])\n",
    "    print('已完成全部信息收集')\n",
    "def main_func_(): #主函数，控制要找的关键词（按日期，不收集用户信息）\n",
    "    keywords=['消费','复工','复产']\n",
    "    all_link=[]\n",
    "    print('收集开始...')\n",
    "    for each in keywords:\n",
    "        print('当前关键词：'+each)\n",
    "        get_keyword_all_inf_by_date(get_all_pages_link_by_date(each),each)\n",
    "    print('已完成全部信息收集')\n",
    "main_func_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
