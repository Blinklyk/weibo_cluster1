{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.models.doc2vec import Doc2Vec, LabeledSentence\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from pprint import pprint\n",
    "import time\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stopword():\n",
    "    #stopword_path = 'F:\\\\编程练习\\\\Jupyter notebook\\\\文本文件夹\\\\博客爬虫分析文章\\\\停用词表.txt'\n",
    "    stopword_path = 'F:\\\\学习用夹\\\\大三下学期\\\\综合课程设计\\\\实验数据\\\\停用词库.txt'\n",
    "    f_stop = open(stopword_path,'r',encoding='UTF-8')\n",
    "    sw = [line.strip() for line in f_stop]\n",
    "    f_stop.close()\n",
    "    stopword_extend = ['\\n','湖北','武汉','病毒','中国',\n",
    "                       '兰州','甘肃','我国','北京','微博','正文',\n",
    "                       '收起','肖战','消费','复工','复产','全文','消费者','疫情','网页','链接']\n",
    "    sw.extend(stopword_extend)\n",
    "    return sw\n",
    "\n",
    "\n",
    "def jieba_text(data_path):\n",
    "    stopwords = load_stopword()\n",
    "    df = pd.read_excel(data_path)\n",
    "    #df = pd.read_csv(data_path)\n",
    "    text_seg_list = []\n",
    "    for index,row in df.iterrows():\n",
    "        #fileId = row['id']\n",
    "        fileContent = row['内容']\n",
    "        segs = jieba.analyse.textrank(fileContent,topK=20,withWeight=False,allowPOS=('ns','n','nr','nt','vn','v')) \n",
    "        segments_list = []\n",
    "        for seg in segs:\n",
    "            if seg not in stopwords and len(seg) > 1:\n",
    "                segments_list.append(seg)\n",
    "        text_seg_list.append(segments_list)\n",
    "    return text_seg_list\n",
    "\n",
    "    \n",
    "#导出csv文件\n",
    "def dataToCsv(file,df):\n",
    "    file_data = df\n",
    "    file_data.to_csv(file,index=False)\n",
    "    print('csv文件已生成在：{}'.format(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA(text_seg_list,topic_num):\n",
    "    # 建立字典\n",
    "    dictionary = corpora.Dictionary(text_seg_list)\n",
    "    V = len(dictionary)\n",
    "\n",
    "    # 转换文本数据为索引，并计数\n",
    "    corpus = [dictionary.doc2bow(text) for text in text_seg_list]\n",
    "\n",
    "    # 计算tf-idf值\n",
    "    corpus_tfidf = models.TfidfModel(corpus)[corpus]\n",
    "\n",
    "    # 训练模型\n",
    "    lda = models.LdaModel(corpus_tfidf, num_topics=topic_num, id2word=dictionary)\n",
    "    #alpha=0.01, eta=0.01, minimum_probability=0.001,update_every=1, chunksize=100, passes=1\n",
    "    Perplexity = lda.log_perplexity(corpus_tfidf)\n",
    "\n",
    "    num_show_term = 10  # 每个主题显示几个词\n",
    "    #print('结果：每个主题的词分布：')\n",
    "    lda_topic = {'1':[],'2':[],'3':[],'4':[],'5':[],'6':[],'7':[],'8':[],'9':[],'10':[]}\n",
    "    lda_topic_prob = []\n",
    "    for topic_id in range(topic_num):\n",
    "        #print('主题#%d：\\t' % topic_id)\n",
    "        term_distribute_all = lda.get_topic_terms(topicid=topic_id)\n",
    "        term_distribute = term_distribute_all[:num_show_term]\n",
    "        term_distribute = np.array(term_distribute)\n",
    "        term_id = term_distribute[:, 0].astype(np.int)\n",
    "        #print('词：\\t', )\n",
    "        i = 0\n",
    "        for t in term_id:\n",
    "            i += 1\n",
    "            #print(dictionary.id2token[t], )\n",
    "            lda_topic[str(i)].append(dictionary.id2token[t])\n",
    "        #print('\\n概率：\\t', term_distribute[:, 1])\n",
    "        lda_topic_prob.append(term_distribute[:, 1])\n",
    "    #print(lda.print_topics(5))\n",
    "    \n",
    "    #lda可视化\n",
    "    vis_data = pyLDAvis.gensim.prepare(lda,corpus_tfidf,dictionary)\n",
    "    pyLDAvis.show(vis_data,open_browser = False)\n",
    "    return lda_topic,lda_topic_prob,Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc2vec与K-means聚类\n",
    "TaggededDocument = gensim.models.doc2vec.TaggedDocument\n",
    " \n",
    "    \n",
    "def get_train(text_seg_list):\n",
    "    x_train = []\n",
    "    for i, text in enumerate(text_seg_list):\n",
    "        document = TaggededDocument(text, tags=[i])\n",
    "        x_train.append(document)\n",
    "    return x_train\n",
    " \n",
    "    \n",
    "def train(x_train, size=200, epoch_num=1):\n",
    "    model_dm = Doc2Vec(x_train, min_count=1, window = 3, size = size, sample=1e-3, negative=5, workers=4)\n",
    "    model_dm.train(x_train, total_examples=model_dm.corpus_count, epochs=100)\n",
    "    #model_dm.save('model/model_dm')\n",
    "    return model_dm\n",
    " \n",
    "    \n",
    "def cluster(x_train,model_dm,n_clu):\n",
    "    infered_vectors_list = []\n",
    "    for text, label in x_train:\n",
    "        vector = model_dm.infer_vector(text)\n",
    "        infered_vectors_list.append(vector)\n",
    " \n",
    "    kmean_model = KMeans(n_clusters=n_clu)\n",
    "    kmean_model.fit(infered_vectors_list)\n",
    "    labels= kmean_model.predict(infered_vectors_list)\n",
    "    return labels,infered_vectors_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_1 = 'F:\\\\学习用夹\\\\大三下学期\\\\综合课程设计\\\\实验数据\\\\复产内容信息2020-05-10.xls'\n",
    "data_path_2 = 'F:\\\\学习用夹\\\\大三下学期\\\\综合课程设计\\\\实验数据\\\\复工内容信息2020-05-10.xlsx'\n",
    "data_path_3 = 'F:\\\\学习用夹\\\\大三下学期\\\\综合课程设计\\\\实验数据\\\\消费内容信息2020-05-10.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\MU_XIA~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.564 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "C:\\Users\\Mu_Xiaobai\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:574: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    }
   ],
   "source": [
    "#建立训练集\n",
    "text_seg_list = jieba_text(data_path_3)\n",
    "x_train = get_train(text_seg_list)\n",
    "model_dm = train(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00079411373\n",
      "0.0014626954\n",
      "-2.145578e-05\n",
      "-0.0038430458\n",
      "-0.022319539\n",
      "-0.025925156\n",
      "-0.026607657\n",
      "-0.033081427\n",
      "-0.033150654\n",
      "-0.032359377\n",
      "-0.036587823\n",
      "-0.033066437\n",
      "-0.03287535\n",
      "-0.0482177\n",
      "-0.04129364\n",
      "-0.03779978\n",
      "-0.038548328\n"
     ]
    }
   ],
   "source": [
    "#轮廓系数评估\n",
    "infered_vectors_list = []\n",
    "for text, label in x_train:\n",
    "    vector = model_dm.infer_vector(text)\n",
    "    infered_vectors_list.append(vector)\n",
    "\n",
    "def get_cluster_n(infered_vectors_list,model_dm,n_clu):\n",
    "    kmean_model = KMeans(n_clusters=n_clu)\n",
    "    kmean_model.fit(infered_vectors_list)\n",
    "    labels = kmean_model.predict(infered_vectors_list)\n",
    "    print(metrics.silhouette_score(infered_vectors_list, kmean_model.labels_, metric='cosine'))\n",
    "\n",
    "#寻找3到20里的轮廓系数，用系数较高的值做聚类中心\n",
    "for i in range(3,20):\n",
    "    get_cluster_n(infered_vectors_list,model_dm,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#根据聚类中心进行K-Means聚类\n",
    "labels,infered_vectors_list = cluster(x_train,model_dm,3)   #第二个系数选择聚类中心\n",
    "clu_df = pd.DataFrame({'内容':text_seg_list})\n",
    "clu_df['labels'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv文件已生成在：F:\\学习用夹\\大三下学期\\综合课程设计\\实验数据\\聚类结果\\0\n",
      "csv文件已生成在：F:\\学习用夹\\大三下学期\\综合课程设计\\实验数据\\聚类结果\\1\n",
      "csv文件已生成在：F:\\学习用夹\\大三下学期\\综合课程设计\\实验数据\\聚类结果\\2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mu_Xiaobai\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:34: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "#结果存入相应文件夹\n",
    "save_path = 'F:\\\\学习用夹\\\\大三下学期\\\\综合课程设计\\\\实验数据\\\\聚类结果\\\\'   #保存聚类结果的文件夹\n",
    "clu_gp = clu_df.groupby('labels')\n",
    "\n",
    "cluster_lists = []\n",
    "for clu_id in clu_gp.groups.keys():\n",
    "    dataToCsv(save_path + str(clu_id), clu_gp.get_group(clu_id)['内容'])  #在保存的csv文件里查看聚类结果好坏\n",
    "    cluster_lists.append(clu_gp.get_group(clu_id)['内容'].values.tolist())   #cluster_lists[n],n为聚类中心数，在lda中用此列表分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "产品          721\n",
      "汽车          638\n",
      "发布          556\n",
      "市场          556\n",
      "公司          468\n",
      "数据          446\n",
      "品牌          443\n",
      "科技          424\n",
      "手机          412\n",
      "问题          348\n"
     ]
    }
   ],
   "source": [
    "#聚类后的词频分析,根据高频词判断聚类质量\n",
    "def word_count(cluster_list):\n",
    "    counts = {}\n",
    "    words = []\n",
    "    for w_list in cluster_list:\n",
    "        for w in w_list:\n",
    "            words.append(w)\n",
    "    for word in words:\n",
    "        counts[word] = counts.get(word, 0) + 1\n",
    "    items = list(counts.items())\n",
    "    items.sort(key=lambda x:x[1], reverse=True)\n",
    "    for i in range(10):\n",
    "        word, count = items[i]\n",
    "        print(\"{0:<10}{1:>5}\".format(word, count))\n",
    "\n",
    "word_count(cluster_lists[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mu_Xiaobai\\Anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: if you're in the IPython notebook, pyLDAvis.show() is not the best command\n",
      "      to use. Consider using pyLDAvis.display(), or pyLDAvis.enable_notebook().\n",
      "      See more information at http://pyLDAvis.github.io/quickstart.html .\n",
      "\n",
      "You must interrupt the kernel to end this command\n",
      "\n",
      "Serving to http://127.0.0.1:8889/    [Ctrl-C to exit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Jun/2020 16:45:26] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jun/2020 16:45:26] \"GET /LDAvis.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jun/2020 16:45:26] \"GET /d3.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Jun/2020 16:45:26] code 404, message Not Found\n",
      "127.0.0.1 - - [17/Jun/2020 16:45:26] \"GET /favicon.ico HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [17/Jun/2020 16:45:26] \"GET /LDAvis.js HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "stopping Server...\n",
      "    1   2     3     4   5    6    7   8   9  10\n",
      "0  汽车  支持    补贴   新能源  发放   政策   鼓励  旅游  发布  措施\n",
      "1  粉丝  理智    季报    力度  降至   消保  消费量  注册  理性  品牌\n",
      "2  单曲  公益    网民    产品  数据   亏损   四川  选项  力量  报告\n",
      "3  还款  知情  金融机构    下调  频道   收费   公益  首付  动能  人士\n",
      "4  景区  歌手    专区    时称  侵权   中央   涉嫌  接受  陷阱  植物\n",
      "5  公司  市场    数据   任正非  华为   科技   充值  业绩  板块  行业\n",
      "6  粉丝  音乐   销售量    理性  投稿   付费   光点  大会  不配  金条\n",
      "7  专辑  全网    小号  消费结构  吉利  重庆市   数据  希望  保护  出行\n",
      "          1         2         3         4         5         6         7  \\\n",
      "0  0.005162  0.004077  0.003940  0.003538  0.003423  0.003261  0.003110   \n",
      "1  0.003388  0.003059  0.002862  0.002803  0.002628  0.002421  0.002413   \n",
      "2  0.005585  0.002807  0.002644  0.002288  0.002066  0.001936  0.001865   \n",
      "3  0.003766  0.003327  0.003220  0.002971  0.002777  0.002370  0.002272   \n",
      "4  0.004251  0.002008  0.002007  0.001947  0.001893  0.001875  0.001869   \n",
      "5  0.003543  0.002949  0.002933  0.002762  0.002621  0.002418  0.002325   \n",
      "6  0.003022  0.002420  0.002131  0.002103  0.002065  0.001897  0.001873   \n",
      "7  0.003422  0.002490  0.002465  0.002171  0.002085  0.001933  0.001827   \n",
      "\n",
      "          8         9        10  \n",
      "0  0.002940  0.002868  0.002684  \n",
      "1  0.002363  0.002271  0.002186  \n",
      "2  0.001836  0.001806  0.001805  \n",
      "3  0.002210  0.002041  0.001937  \n",
      "4  0.001780  0.001764  0.001720  \n",
      "5  0.002299  0.002248  0.002205  \n",
      "6  0.001827  0.001763  0.001729  \n",
      "7  0.001796  0.001746  0.001725  \n"
     ]
    }
   ],
   "source": [
    "#对相应文件做lda分析\n",
    "lda_topic,lda_topic_prob,Perplexity = LDA(cluster_lists[0],8)    #第二个系数为lda主题个数\n",
    "df_topic = pd.DataFrame(lda_topic)\n",
    "df_topic_prob = pd.DataFrame(lda_topic_prob,columns=['1','2','3','4','5','6','7','8','9','10'])\n",
    "print(df_topic)\n",
    "print(df_topic_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
