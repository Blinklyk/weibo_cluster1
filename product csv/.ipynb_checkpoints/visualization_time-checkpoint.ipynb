{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "import pandas as pd\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.models.doc2vec import Doc2Vec, LabeledSentence\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from pprint import pprint\n",
    "import time\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(file_name):\n",
    "    f = open(file_name, 'r', encoding='utf-8')\n",
    "    content = f.read()\n",
    "    final_list = list()\n",
    "    rows = content.split('\\n')\n",
    "    for row in rows:\n",
    "        final_list.append(row.split(','))\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'read_csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-0b6dba04fa0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'jieba_text_xiaofei1_6_4w.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'read_csv' is not defined"
     ]
    }
   ],
   "source": [
    "text = read_csv('jieba_text_xiaofei1_6_4w.csv')\n",
    "print(text.shape())\n",
    "text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster1 = {'id':[], 'text':[]}\n",
    "cluster2 = {'id':[], 'text':[]}\n",
    "cluster3 = {'id':[], 'text':[]}\n",
    "cluster4 = {'id':[], 'text':[]}\n",
    "cluster5 = {'id':[], 'text':[]}\n",
    "cluster6 = {'id':[], 'text':[]}\n",
    "cluster7 = {'id':[], 'text':[]}\n",
    "cluster8 = {'id':[], 'text':[]}\n",
    "cluster9 = {'id':[], 'text':[]}\n",
    "cluster10 = {'id':[], 'text':[]}\n",
    "cluster11 = {'id':[], 'text':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cluster():\n",
    "    '''\n",
    "    加载聚类表\n",
    "    :return: 返回聚类表的列表\n",
    "    '''\n",
    "    cluster_path = 'data/主题选择2.txt'\n",
    "    cluster_f = [line.strip() for line in codecs.open('data/主题选择.txt', 'r', 'utf-8').readlines()]\n",
    "    cw_list = []\n",
    "    for line in cluster_f:\n",
    "        cw_list.append(line)\n",
    "    return cw_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(a, b):\n",
    "    \"\"\"\n",
    "    Calculate Jaccard similarity.\n",
    "    :return: similar score\n",
    "    \"\"\"\n",
    "    a = set(a)\n",
    "    b = set(b)\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['旅游', '旅馆', '旅客', '入园', '交通', '门票', '机票'],\n",
       " ['医疗', '器械', '医用', '医药', '口罩'],\n",
       " ['娱乐', '粉丝', '明星', '偶像'],\n",
       " ['餐饮', '火锅', '零食', '猪肉', '食品', '饮料'],\n",
       " ['购物', '购车', '买房', '夜市'],\n",
       " ['A股', '个股', '股市', '股份', '创业板', '基金']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 显示聚类主题\n",
    "clusters = load_cluster()\n",
    "clusters_list = []\n",
    "for cluster in clusters:\n",
    "    cluster = cluster.split(',')\n",
    "    clusters_list.append(cluster)\n",
    "clusters_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义函数 jaccard系数最大的词划分到那一类\n",
    "def get_score():\n",
    "    text_line_id = 1\n",
    "    for text_line in text:\n",
    "        cluster_class = 0\n",
    "        cluster_js = 1\n",
    "#         print(text_line)\n",
    "        for cluster in clusters_list:\n",
    "            max_score = 0\n",
    "            score = jaccard(text_line, cluster)\n",
    "#             print(score)\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                cluster_class = cluster_js\n",
    "            cluster_js += 1\n",
    "        else:\n",
    "            if cluster_class == 1:\n",
    "                cluster1['id'].append(text_line_id)\n",
    "                cluster1['text'].append(text_line)\n",
    "            if cluster_class == 2:\n",
    "                cluster2['id'].append(text_line_id)\n",
    "                cluster2['text'].append(text_line)\n",
    "            if cluster_class == 3:\n",
    "                cluster3['id'].append(text_line_id)\n",
    "                cluster3['text'].append(text_line)\n",
    "            if cluster_class == 4:\n",
    "                cluster4['id'].append(text_line_id)\n",
    "                cluster4['text'].append(text_line)\n",
    "            if cluster_class == 5:\n",
    "                cluster5['id'].append(text_line_id)\n",
    "                cluster5['text'].append(text_line)\n",
    "            if cluster_class == 6:\n",
    "                cluster6['id'].append(text_line_id)\n",
    "                cluster6['text'].append(text_line)\n",
    "            if cluster_class == 7:\n",
    "                cluster7['id'].append(text_line_id)\n",
    "                cluster7['text'].append(text_line)\n",
    "            if cluster_class == 8:\n",
    "                cluster8['id'].append(text_line_id)\n",
    "                cluster8['text'].append(text_line)\n",
    "            if cluster_class == 9:\n",
    "                cluster9['id'].append(text_line_id)\n",
    "                cluster9['text'].append(text_line)\n",
    "            if cluster_class == 10:\n",
    "                cluster10['id'].append(text_line_id)\n",
    "                cluster10['text'].append(text_line)\n",
    "            if cluster_class == 11:\n",
    "                cluster11['id'].append(text_line_id)\n",
    "                cluster11['text'].append(text_line)\n",
    "        text_line_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "868\n"
     ]
    }
   ],
   "source": [
    "print(len(cluster1['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3898\n"
     ]
    }
   ],
   "source": [
    "print(len(cluster2['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1883\n"
     ]
    }
   ],
   "source": [
    "print(len(cluster3['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5044\n"
     ]
    }
   ],
   "source": [
    "print(len(cluster4['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3448\n"
     ]
    }
   ],
   "source": [
    "print(len(cluster5['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4550\n"
     ]
    }
   ],
   "source": [
    "print(len(cluster6['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(cluster7['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def LDA(text_seg_list,topic_num):\n",
    "#     # 建立字典\n",
    "#     dictionary = corpora.Dictionary(text_seg_list)\n",
    "#     V = len(dictionary)\n",
    "\n",
    "#     # 转换文本数据为索引，并计数\n",
    "#     corpus = [dictionary.doc2bow(text) for text in text_seg_list]\n",
    "\n",
    "#     # 计算tf-idf值\n",
    "#     corpus_tfidf = models.TfidfModel(corpus)[corpus]\n",
    "\n",
    "#     # 训练模型\n",
    "#     lda = models.LdaModel(corpus_tfidf, num_topics=topic_num, id2word=dictionary)\n",
    "#     #alpha=0.01, eta=0.01, minimum_probability=0.001,update_every=1, chunksize=100, passes=1\n",
    "#     Perplexity = lda.log_perplexity(corpus_tfidf)\n",
    "\n",
    "#     num_show_term = 10  # 每个主题显示几个词\n",
    "#     #print('结果：每个主题的词分布：')\n",
    "#     lda_topic = {'1':[],'2':[],'3':[],'4':[],'5':[],'6':[],'7':[],'8':[],'9':[],'10':[]}\n",
    "#     lda_topic_prob = []\n",
    "#     for topic_id in range(topic_num):\n",
    "#         #print('主题#%d：\\t' % topic_id)\n",
    "#         term_distribute_all = lda.get_topic_terms(topicid=topic_id)\n",
    "#         term_distribute = term_distribute_all[:num_show_term]\n",
    "#         term_distribute = np.array(term_distribute)\n",
    "#         term_id = term_distribute[:, 0].astype(np.int)\n",
    "#         #print('词：\\t', )\n",
    "#         i = 0\n",
    "#         for t in term_id:\n",
    "#             i += 1\n",
    "#             #print(dictionary.id2token[t], )\n",
    "#             lda_topic[str(i)].append(dictionary.id2token[t])\n",
    "#         #print('\\n概率：\\t', term_distribute[:, 1])\n",
    "#         lda_topic_prob.append(term_distribute[:, 1])\n",
    "#     print(lda.print_topics(5))\n",
    "    \n",
    "#     #lda可视化\n",
    "#     vis_data = pyLDAvis.gensim.prepare(lda,corpus_tfidf,dictionary)\n",
    "#     pyLDAvis.show(vis_data,open_browser = False)\n",
    "#     return lda_topic,lda_topic_prob,Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导出csv文件\n",
    "def dataToCsv(file,df):\n",
    "    file_data = df\n",
    "    file_data.to_csv(file,index=False)\n",
    "    print('csv文件已生成在：{}'.format(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA(text_seg_list_dic,topic_num):\n",
    "    text_seg_list = text_seg_list_dic['text']\n",
    "    # 建立字典\n",
    "    dictionary = corpora.Dictionary(text_seg_list)\n",
    "    V = len(dictionary)\n",
    "\n",
    "    # 转换文本数据为索引，并计数\n",
    "    corpus = [dictionary.doc2bow(text) for text in text_seg_list]\n",
    "\n",
    "    # 计算tf-idf值\n",
    "    corpus_tfidf = models.TfidfModel(corpus)[corpus]\n",
    "\n",
    "    # 训练模型\n",
    "    lda = models.LdaModel(corpus_tfidf, num_topics=topic_num, id2word=dictionary)\n",
    "    #alpha=0.01, eta=0.01, minimum_probability=0.001,update_every=1, chunksize=100, passes=1\n",
    "    Perplexity = lda.log_perplexity(corpus_tfidf)\n",
    "\n",
    "    num_show_term = 10  # 每个主题显示几个词\n",
    "    #print('结果：每个主题的词分布：')\n",
    "    lda_topic = {'1':[],'2':[],'3':[],'4':[],'5':[],'6':[],'7':[],'8':[],'9':[],'10':[]}\n",
    "    lda_topic_prob = []\n",
    "    for topic_id in range(topic_num):\n",
    "        #print('主题#%d：\\t' % topic_id)\n",
    "        term_distribute_all = lda.get_topic_terms(topicid=topic_id)\n",
    "        term_distribute = term_distribute_all[:num_show_term]\n",
    "        term_distribute = np.array(term_distribute)\n",
    "        term_id = term_distribute[:, 0].astype(np.int)\n",
    "        #print('词：\\t', )\n",
    "        i = 0\n",
    "        for t in term_id:\n",
    "            i += 1\n",
    "            #print(dictionary.id2token[t], )\n",
    "            lda_topic[str(i)].append(dictionary.id2token[t])\n",
    "        #print('\\n概率：\\t', term_distribute[:, 1])\n",
    "        lda_topic_prob.append(term_distribute[:, 1])\n",
    "    #print(lda.print_topics(5))\n",
    "    \n",
    "    #lda可视化\n",
    "    vis_data = pyLDAvis.gensim.prepare(lda,corpus_tfidf,dictionary)\n",
    "#     pyLDAvis.show(vis_data,open_browser = False)\n",
    "    \n",
    "    #输出每个list所属的类别\n",
    "    topic_list=[]\n",
    "    for topics in lda.get_document_topics(corpus)[:]:\n",
    "        #print(topics)\n",
    "        listj=[]\n",
    "        for j in topics:\n",
    "            listj.append(j[1])\n",
    "        bz=listj.index(max(listj))\n",
    "        topic_list.append(topics[bz][0])\n",
    "    corpus_topic_df = pd.DataFrame({'id':text_seg_list_dic['id'], 'topic':topic_list, 'content':text_seg_list})\n",
    "    dataToCsv('data/股票基金簇.csv',corpus_topic_df)\n",
    "    \n",
    "    return lda_topic,lda_topic_prob,Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ANACONDA\\envs\\statistic\\lib\\site-packages\\pyLDAvis\\_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv文件已生成在：data/股票基金簇.csv\n"
     ]
    }
   ],
   "source": [
    "# 生成簇结合时间分析\n",
    "text_dic = cluster6\n",
    "\n",
    "lda_topic,lda_topic_prob,Perplexity = LDA(text_dic,3)    #第二个系数为lda主题个数\n",
    "df_topic = pd.DataFrame(lda_topic)\n",
    "df_topic_prob = pd.DataFrame(lda_topic_prob,columns=['1','2','3','4','5','6','7','8','9','10'])\n",
    "#print(df_topic)\n",
    "#print(df_topic_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:statistic]",
   "language": "python",
   "name": "conda-env-statistic-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
